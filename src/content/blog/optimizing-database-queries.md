---
title: "Database Query Optimization: From Slow to Lightning Fast"
date: "2024-01-08" 
readTime: "8 min read"
excerpt: "Deep dive into database query optimization techniques, indexing strategies, and performance monitoring."
tags: ["database", "optimization", "sql", "performance"]
---

# Database Query Optimization: From Slow to Lightning Fast

Database performance can make or break your application. Here's how we turned our slowest queries into lightning-fast operations.

## The Problem

Our dashboard was loading in **8+ seconds**. Users were frustrated, and our servers were overwhelmed. The culprit? Poorly optimized database queries.

```sql
-- This query was killing our performance
SELECT u.*, p.*, o.*, COUNT(oi.id) as item_count
FROM users u
LEFT JOIN profiles p ON u.id = p.user_id  
LEFT JOIN orders o ON u.id = o.user_id
LEFT JOIN order_items oi ON o.id = oi.order_id
WHERE u.created_at > '2024-01-01'
GROUP BY u.id, p.id, o.id;
```

**Execution time**: 8.2 seconds ðŸ˜±

## Diagnosis Tools

### 1. Query Execution Plans

```sql
EXPLAIN ANALYZE
SELECT u.*, p.*, o.*, COUNT(oi.id) as item_count
FROM users u
LEFT JOIN profiles p ON u.id = p.user_id  
-- ... rest of query
```

**Key findings**:
- Sequential scan on users table (1M+ rows)
- Nested loop joins without proper indexes
- Unnecessary data being selected

### 2. Performance Monitoring

We use **pgHero** for PostgreSQL monitoring:

```bash
# Install pgHero for Rails apps
gem 'pghero'

# Check slow queries
PgHero.slow_queries
```

## Optimization Strategies

### 1. Index Optimization

#### Before: No indexes
```sql
-- No indexes on critical columns
SELECT * FROM orders WHERE user_id = 123 AND status = 'completed';
-- Execution: 2.3s (sequential scan)
```

#### After: Strategic indexing
```sql
-- Composite index for common query patterns
CREATE INDEX idx_orders_user_status ON orders(user_id, status);
CREATE INDEX idx_users_created_at ON users(created_at);

-- Now the same query:
SELECT * FROM orders WHERE user_id = 123 AND status = 'completed';
-- Execution: 0.003s (index scan) ðŸš€
```

### 2. Query Restructuring

#### Before: N+1 Problem
```python
# This generates 1000+ queries
users = User.objects.all()
for user in users:
    print(user.profile.bio)  # Each access hits the database
    print(user.orders.count())  # Another query per user
```

#### After: Efficient Loading
```python
# Single query with joins and prefetch
users = User.objects.select_related('profile') \
                   .prefetch_related('orders') \
                   .all()

for user in users:
    print(user.profile.bio)     # No additional query
    print(user.orders.count())  # Prefetched, no query
```

### 3. Query Optimization

#### Original Slow Query
```sql
SELECT u.*, p.*, o.*, COUNT(oi.id) as item_count
FROM users u
LEFT JOIN profiles p ON u.id = p.user_id  
LEFT JOIN orders o ON u.id = o.user_id
LEFT JOIN order_items oi ON o.id = oi.order_id
WHERE u.created_at > '2024-01-01'
GROUP BY u.id, p.id, o.id;
```

#### Optimized Version
```sql
-- Split into focused queries
-- 1. Get users efficiently
SELECT u.id, u.email, u.created_at, p.bio, p.avatar_url
FROM users u
INNER JOIN profiles p ON u.id = p.user_id  
WHERE u.created_at > '2024-01-01'
AND u.active = true;

-- 2. Get order counts separately
SELECT user_id, COUNT(*) as order_count, 
       SUM(total_items) as total_items
FROM orders 
WHERE user_id IN (/* user IDs from above */)
GROUP BY user_id;
```

**Result**: 8.2s â†’ **0.05s** âš¡

## Advanced Techniques

### 1. Materialized Views

For complex aggregations that don't change often:

```sql
-- Create materialized view
CREATE MATERIALIZED VIEW user_order_summary AS
SELECT 
    u.id,
    u.email,
    COUNT(o.id) as order_count,
    SUM(o.total_amount) as lifetime_value,
    MAX(o.created_at) as last_order_date
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.email;

-- Refresh periodically
REFRESH MATERIALIZED VIEW user_order_summary;
```

### 2. Connection Pooling

```python
# Database connection pool settings
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'myapp',
        'OPTIONS': {
            'MAX_CONNS': 20,        # Maximum connections
            'MIN_CONNS': 5,         # Minimum connections  
            'CONNECTION_LIFETIME': 300,  # 5 minutes
        }
    }
}
```

### 3. Query Caching

```python
from django.core.cache import cache

def get_user_stats(user_id):
    cache_key = f"user_stats_{user_id}"
    stats = cache.get(cache_key)
    
    if not stats:
        stats = {
            'order_count': Order.objects.filter(user_id=user_id).count(),
            'total_spent': Order.objects.filter(user_id=user_id)
                                      .aggregate(Sum('total_amount')),
        }
        cache.set(cache_key, stats, timeout=3600)  # Cache for 1 hour
    
    return stats
```

## Monitoring & Alerting

### 1. Performance Metrics

```python
# Custom Django middleware for query monitoring
class QueryCountMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request):
        queries_before = len(connection.queries)
        response = self.get_response(request)
        queries_after = len(connection.queries)
        
        query_count = queries_after - queries_before
        if query_count > 10:  # Alert threshold
            logger.warning(f"High query count: {query_count} queries")
            
        return response
```

### 2. Slow Query Alerts

```sql
-- PostgreSQL: Log slow queries
log_min_duration_statement = 1000  # Log queries > 1 second

-- MySQL: Enable slow query log
slow_query_log = 1
long_query_time = 1
```

## Results & Impact

After optimization:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Page Load Time** | 8.2s | 0.3s | **96% faster** |
| **Database CPU** | 85% | 25% | **70% reduction** |
| **Concurrent Users** | 50 | 500 | **10x increase** |
| **Query Count** | 1,247 | 12 | **99% reduction** |

## Best Practices

### 1. Prevention
- Always use `EXPLAIN ANALYZE` for new queries
- Set up query performance monitoring from day one
- Regular database maintenance (VACUUM, ANALYZE)

### 2. Development
```python
# Use Django Debug Toolbar in development
if DEBUG:
    INSTALLED_APPS += ['debug_toolbar']
    MIDDLEWARE += ['debug_toolbar.middleware.DebugToolbarMiddleware']
```

### 3. Testing
```python
# Test query counts in your tests
from django.test.utils import override_settings
from django.db import connection

@override_settings(DEBUG=True)
def test_query_count(self):
    with self.assertNumQueries(1):  # Ensure only 1 query
        list(User.objects.select_related('profile').all())
```

## Tools We Use

- **pgHero** - PostgreSQL performance monitoring
- **Django Debug Toolbar** - Development query analysis  
- **New Relic** - Production APM monitoring
- **pgBadger** - PostgreSQL log analyzer

## Next Up

In our next post: **"Implementing Database Sharding at Scale"** - how we partitioned 100M+ records across multiple databases.

---

*Need help with database optimization? Let's connect on [LinkedIn](https://linkedin.com/in/yagizkilicarslan)!* 